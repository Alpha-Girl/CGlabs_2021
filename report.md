# 人脸动画及表情合成技术发展综述

## 1 引言

具有真实感的人脸脸表情动画合成一直是计算机图形学和计算机视觉领域的研究热点和难点之一，且被广泛应用于数字娱乐、视频会议、医疗、辅助教育等领域。在人脸动画领域，高端方法和低端方法之间存在很大的差距。高端方法生成的面部动画令真正的人类难以区分，但是需要人脸建模的专家付出大量的体力劳动。而低端方法，依托传感器的面部捕捉等，得到人脸动画参数，生成人类动画，又不足以表现人脸自然变化中的细节。为此，研究者们不断钻研，在鲁棒性，性能，易用性，实时性等方面得到了优化。

目前，人脸表情合成方法主要分为三大类：基于几何模型（三维网格模型）的人脸表情合成，基于二维图像的人脸表情合成和基于深度神经网络的人脸表情合成。随着应用场景不断拓宽，人脸表情动画的驱动方式也不断增加，从最初的参数驱动，到图像数据驱动，再到音频，文本驱动。

## 2 基于几何模型

基于三维网格的方法是通过跟踪一系列人脸表情变化的曲线和基于全局统计的模型生成所需的目标图像。

基于这种方法，有以下这些主要成果：Pighin等提出了具有逼真纹理的三维人脸建模的表情合成系统。Wang等构建了一个MPEG-4兼容的人脸动画系统，使用脸部定义参数（ facial definition parameter, FDP）来构建人脸模型，通过脸部动画参数（ facial animation parameter, FAP）来驱动该模型生成各种表情。Blanz等提出了从单张图片或视频中恢复人脸的三维模型，尝试从带纹理的三维模型中进行表情合成，但创建一个好的模型相当困难，因为必须对面部的所有细节进行建模，如眼睛、头发、牙齿等。Vlasic等提出的多线性模型（Multilinear Models）将分离参数化不同的属性（如顶点信息、形状、视位、表情 等）建立在同一个数据张量空间中， 这些属性之间用笛卡尔积来构造并且相互独立。将这些独立属性参数进行任意组合，最终得到不同的人脸表情。Lv等提出一种面向同一人脸表情转移的方法，即对目标人脸进行三维建模，生成特定的混合形状（blendshape）模型，利用该模型生成与输入人脸图像匹配的三维人脸模型，并对图像进行扭曲融合，生成所需的目标人脸图像。

尽管这些方法较为稳定，且便于计算，但这些方法通常不允许西粒度变形，并且在困难区域（嘴巴，眼睛）难以生成逼真的效果。而人们往往是通过这些五官特征来区分人脸的。为了解决五官特征不明显，不真实的问题，人们又基于分而治之的思想，提出了对五官，皱纹等细节部位先进行单独处理，再通过一定的约束合成人脸模型，以提高人脸的真实感。

Pasquariello等在三维人脸模型 中加入皱纹等细节因素，将人脸模型进行网格化，并按照人的生理结构将人脸分成嘴、眼睛、眉毛、额头等区域，使得每一区域的网格数量和拓扑等都不同，进而实现表情的模拟。Zhang等在三维形变模型中也加入了皱纹等细节，将其划分为14个子区域，避免了表情皱纹超过分区的边界。Joshi等提出的基于物理的分割方法可以自动将人脸分割成多个区域，每一个区域都表示成混合形状的 线 性 组 合，从混合形状中学习约 束条件和参数。Park等将给定的每一个源关键模型分成３个子区域，每一个子区域都包含人脸的关键特征，实现了面部表情的合成。Joshi把每一个表情看成 其他表情的线性组合，通过改变这些线性组合的权重来合成比较完整的面部表情。Garrido提出了一种基于图像的人脸视频再现方法，对于输入的两个不同人脸的面部表情视频， 将源序列的表情传递给目标序列，同时尽可能地保留灯光、背景等因素。该方法的缺点是：需要一些特定的人脸数据库，且依赖于输入图像的一些表情信息，如是否是中性人脸或者有无标记点等等。文献［１２－１３］首先对人脸进行建模，然后得到特定用户的 混合形状模 型，进而求解出 混合形状系数， 最后通过对系数的改变来合成目标表情。Huang等提出了基于非联合学习的人脸表情合成方法：通过一种无监督回归的算法，将具有相同属性的三维人脸模型映射到同一个低维空间，对其进行重建，实现人脸表情的合成。

## 3 基于二维图像

基于二维图像的方法是用已知的表情数据来合成新的表情，或者直接将已有的表情传递到目标图像上。Williams提出通过表情映射的方法来合成新的目标表情，首先提取两幅图像不同的面部特征，然后计算特征之间的矢量差， 最后利用特征向量来进行图像的扭曲。该方法的鲁棒性不足，虽然实现了不同表情之间的转换，但是不适用于戴眼镜和头部位姿有较大变化的情况。Yang等提出了基于表情流的方法：首先提取两幅图像的特征点并分别进行三维人脸重建；然后计算两个三维模型之间的差异，将差异映射到二维图像上得到表情流，再利用所得表情流进行图像扭曲；最后进行图像融合［１７］。 表情流的计算方法往往比较复杂，鲁棒性很差，得到的效果不是很逼真，而且只能在同一人脸之间进行，不具有普遍性。文献［１８－１９］提出了一种人脸图像自动替换系统：首先对输入图 像进行人脸检测；然后将提取的每个人脸进行对齐，并从大量的人脸数据库中找到与其相近的人脸；最后通过图像融合实现目标输入图像的表情合成。这些方法可以有效、快捷地合成人脸表情，但由于人脸具有特异性，且表情复杂、丰富，在表情合成过程中合成具有真实感的表情比较困难。

## 4 基于深度神经网络

基于深度神经网络的方法是在上述两种方法的基础上，结合深度神经网络，提供一种端到端的人脸表情生成方法。这些方法相较传统方法在实时性，鲁棒性等方面有所提高。

Li等提出了FLAME（Faces Learned with an Articulated Model and Expressions）模型，通过从数千个精确对齐的3D扫描中学习人脸模型。该模型以下巴，脖子，眼球，姿态相关校正等线性组合而成，模块化处理，可以对各个部分进行精细化处理，并且最终得到的是低维表示。

Sanyal等在FLAME模型的基础上假设脸部形状是恒定的，与表情，状态，光线等无关，实现了从单张2D图像得到FLAME模型的神经网络。这种方法进一步摆脱了对3D扫描的依赖，是一种无3D监督的学习，使得模型构建更为简便，还维持了很好的鲁棒性。

Paier等提出了一种混合动画空间，该框架利用深度学习来提供一个交互式动画引擎，可以提供简单直观的可视化将其用于面部表情编辑。此外，该框架还训练了这个变分式自动编码器，以学习用于交互式面部动画的人脸表情的低维潜在空间。

Hai等提供了一个用原始波形进行实时语音驱动生成3D人脸动画的深度学习框架。该网络直接映射输入语音序列到一系列面部动作单元激活和头部旋转用以驱动3D混合形状人脸模型。该模型还能学习语音的上下文中的潜在情感变化，使得人脸动画的情绪强度与语音的情绪强度相当。

给定文本作为输入时，Yu等提供了采用深度学习方法合成高真实感、认以身份以及唇音同步的人类动画。

### 参考文献



